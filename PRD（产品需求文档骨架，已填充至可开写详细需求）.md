# PRD（产品需求文档骨架，已填充至可开写详细需求）

> 产品：Inspection Agent v1
> 目标：对 AI 生成文本进行系统化校验与修复，支持“多模型对比”和“AI 评审团”两种模式，输出可溯源的改进建议与修订稿
> 文档状态：准备“需求深挖 & 原型评审”会，已达可进入详细 PRD/SRS 编写粒度

---

## 1. 背景与目标

* **现状痛点**：AI 生成文本质量波动；事实/风格/合规风险难以提前发现；多人评审耗时不一致；缺乏客观评分与可追溯问题定位。
* **产品愿景**：让“生成→校验→修复→复检”的闭环像 CI 一样稳定可控；以“差异显性化+多角色合议+自动修复”为抓手，显著提升出稿通过率与用稿信心。
  * 首稿通过率提升 ≥30%
  * 交付周转时长下降 ≥25%
  * 关键错误（事实/合规）漏报率 ≤5%

## 2. 用户与场景
* **关键使用场景**
  1. 发布前体检：平台合规/事实核查/风格统一
  2. 跨模型择优：相同 Prompt 在 2–3 模型对比、取长补短
  3. 多视角评审：“产品/架构/运营/HR/法务”等合议式改进
  4. 批量复检：大规模内容/多语言版本一致性巡检

## 3. 范围界定
* 两种模式：多模型对比（≤3 模型）、AI 评审团（预置 6–8 角色 + 自定义）
* 检测器：一致性、可读性/结构、敏感/伦理、基础合规、重复度
* 报告导出（MD/PDF/DOCX），Auto-Fix（保守/平衡）+ 轻量复检

## 4. 关键功能

### 4.1 输入与上下文管理
* **需求**：粘贴文本/上传 DOCX/MD/JSON/URL 导入；Prompt+System+Few-shot+术语表与规则绑定；长文自动分块、保段落层级。
* **验收**：
  * 支持≥ 200k 字符，分块可在结果页定位到原文位置
  * 导入 5 秒内完成解析（10MB 文档）
  * 显式记录版本与哈希，保证可重放

### 4.2 多模型对比（Compare）
* **需求**：同一上下文在 ≤3 LLM 并行生成；字面/语义双通道 Diff；差异归因（事实/立场/结构/风格/截断/随机性）。
* **验收**：
  * 三列对比视图（A / 推荐合成稿 / B；第三模型可切换）
  * 句/词级高亮与“语义等价/冲突”标识
  * 每个冲突点提供“修复建议 + 选用理由”

### 4.3 AI 评审团（Jury）
* **需求**：从角色库（产品、架构、法务/合规、HR、营销/小红书、抖音/运营、编辑/风格、SEO）中自动/手动选 3–7 位；统一评分维度与 JSON 输出；生成 CQS 总分与雷达图。
* **验收**：
  * 角色意见结构化：问题→证据→建议→示例改写→严重度
  * 同类问题合并聚类，显示“共识度/少数意见”
  * CQS 与各维度分可复算（权重透明）

### 4.4 检测器（Detectors）
* **需求**：可插拔，至少包含一致性、可读性/结构、敏感/伦理、基础合规、重复度；对“合成推荐稿”和“原稿/候选稿”均可运行。
* **验收**：
  * 句级定位与证据片段
  * 开关与阈值可配置
  * 导出报告含问题索引与定位锚点

### 4.5 Auto-Fix 修复
* **需求**：逐问题一键修复；保守/平衡两档；修订稿 Diff；轻量复检（关键维度回测）。
* **验收**：

  * 修订稿 CQS 平均 +10 分；无新增“高风险”问题
  * 可回滚到任意修订版本

### 4.6 报告与导出
* **需求**：详版/速览版/合规专版模板；水印/密级；JSON 导出供系统对接。
* **验收**：
  * 报告含：总分与雷达、问题清单、示例修复、合成推荐稿
  * 10 万字报告导出 ≤ 8s

### 4.7 协作与权限
* **需求**：评论/批注、@ 人、审签流（草稿→评审→定稿）、操作审计。
* **验收**：

  * 全量操作可追溯，导出带水印与签名摘要

## 5. 端到端业务流程（详细）

### 5.1 多模型对比流程（详）
1. 新建作业 → 输入文本/Prompt → 选择 1–3 模型 → 固定温度/seed → 启动
2. 并行生成，记录用量/延时；失败模型自动重试 2 次，仍失败则提示并继续
3. 标准化文本（标点/空白/段落树），字面 Diff + 语义匹配
4. 差异分类：事实/结论/结构/风格/随机；生成冲突清单与归因
5. 运行检测器（候选稿 + 合成推荐稿）
6. 输出：三列对比 + 冲突列表 + 推荐合成稿 + 修复建议
7. 可选：一键 Auto-Fix → 轻量复检 → 新版分数对比
8. 导出/归档

### 5.2 AI 评审团流程（详）
1. 新建作业 → 输入文本/目标/平台 → 自动推荐评审角色（可手调）
2. 组装统一评审输入（文本/目标/术语/规则/维度）
3. 并行评审：每角色产出结构化 JSON
4. 聚合：按问题聚类，计算共识度；打 CQS（权重透明）
5. 输出：角色意见 Tab、雷达图、问题清单、改写示例、整体建议
6. 可选：Auto-Fix → 轻量复检
7. 导出/归档

### 5.3 失败与异常分支
* 任一模型失败：降级为双模型；页面提示“缺失结果，不影响报告生成”
* 评审员超时：以完成者汇总，标记“缺席”；支持“一键补跑”
* 成本超限：弹窗允许“缩减评审/关闭部分检测器/降采样”

## 6. 交互与信息架构（IA）
* **页面**：主页/新建作业 → 运行中（进度+用量） → 结果页（Compare/Jury 两套） → 报告导出
* **结果页—Compare**：左（模型 A）/中（合成推荐稿）/右（模型 B）；问题面板（筛选/跳转）
* **结果页—Jury**：顶部总分与雷达；中部角色 Tab；右侧问题面板；底部修订稿/对比

## 7. 指标与质控
* CQS、关键错误召回率、平均修复提升分（ΔCQS）、平均周转时长、成本/千字
* 抽检标注集离线评估（每月）
